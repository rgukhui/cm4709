{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgukhui/cm4709/blob/main/Lab06_(PyTorch_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RFdDzWyzWi0"
      },
      "source": [
        "#CM4709 Computer Vision\n",
        "#Lab 06 Object Detection using YOLO\n",
        "\n",
        "##Aims\n",
        "1. Use YOLO for object detection.\n",
        "\n",
        "##Uploading Testing Images\n",
        "We will need some testing images.\n",
        "There are a few in Moodle. You can also add some of your own.\n",
        "Upload them to a folder in your GoogleDrive. e.g. `cm4709/Lab06/data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5t_enwlzwEY"
      },
      "source": [
        "##Mounting GoogleDrive\n",
        "\n",
        "It is faster and easier uploading to GoogleDrive than to a Colab runtime.\n",
        "The following standard code connects your GoogleDrive space to Colab.\n",
        "After this, your image folder should be visible in your Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuoceQ38iJqN",
        "outputId": "a6e79f43-2bc3-42a8-f516-723ca13e941c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2fUFnUXz4kz"
      },
      "source": [
        "##Install YOLO v8\n",
        "\n",
        "There are a few implementation of YOLO:\n",
        "1. [Darknet](https://pjreddie.com/darknet/): This is the official release and is the fastest.\n",
        "1. [Darkflow](https://github.com/thtrieu/darkflow): This is the Tensorflow version of Darknet.\n",
        "1. [OpenCV](https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html): This is the OpenCV Implementation of YOLO. As YOLO is a CNN, you can also export a trained YOLO network to a ONNX file and run it in OpenCV C++. This is useful if you want to deploy a trained YOLO network to another platform.\n",
        "1. [PyTorch](https://docs.ultralytics.com/): Ultralytics has released multiple versions of YOLO in Python. If you want the most up-to-date version, this will be the one to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDtWaF7mjDZn"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#install YOLO v8\n",
        "#\n",
        "!pip install ultralytics\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWd3Pg_qz2iN"
      },
      "source": [
        "##Object Detection Using Command Line\n",
        "\n",
        "The following command line loads the pretrained COCO `yolov8s.pt` weights and detects objects in an image.\n",
        "Output will be saved in the folder `runs/detect/predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WavaBOUZ1Jkw"
      },
      "outputs": [],
      "source": [
        "!yolo predict model=yolov8s.pt source='/content/gdrive/MyDrive/cm4709/Lab06/data/mumbai-traffic.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFHKIM-pLXEE"
      },
      "source": [
        "##Seeing the COCO Classes\n",
        "\n",
        "The YOLO weights we loaded are trained with the [COCO dataset](https://cocodataset.org/). The COCO class names are in the result of detection.\n",
        "The following code does object detecton on a testing image and print out the number of classes and the class labels.\n",
        "\n",
        "The result documentation is [here](https://docs.ultralytics.com/modes/predict/#working-with-results). Note the followings:\n",
        "1. We can feed in multiple images for detection.\n",
        "1. The class names are in the `names` property in each detection result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxVtLvVTMDCN"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "#our 3 testing images\n",
        "image1='/content/gdrive/MyDrive/cm4709/Lab06/data/mumbai-traffic.jpg'\n",
        "image2='/content/gdrive/MyDrive/cm4709/Lab06/data/test.jpg'\n",
        "image3='/content/gdrive/MyDrive/cm4709/Lab06/data/bluetit.png'\n",
        "images=[image1,image2,image3]\n",
        "\n",
        "model = YOLO('yolov8s.pt')  # load a pretrained YOLOv8s detection model\n",
        "results=model.predict(images,agnostic_nms=True,iou=0.6)  # predict on images\n",
        "result=results[0]     #get 1st result\n",
        "classNames=result.names #get class names\n",
        "\n",
        "print('No. of classes: ',len(classNames))\n",
        "print(classNames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFR27YrXokHS"
      },
      "source": [
        "##Understanding the Detection Result\n",
        "\n",
        "The Ultralytics library's detection function accepts multiple images in a list, and the results are returned as another list. Each result contains information of the class confidence score and bounding boxes. You can find documentation of the result [here](https://docs.ultralytics.com/modes/predict/).\n",
        "\n",
        "The following code examines structure of the detection result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C52WsnnXMksC"
      },
      "outputs": [],
      "source": [
        "#Note: variable \"results\" is instantiated above\n",
        "\n",
        "print('No. of results: ',len(results))\n",
        "\n",
        "#go through all results\n",
        "for result in results:\n",
        "  boxes=result.boxes                    #get all boxes in this result\n",
        "  print('  No. of boxes: ',len(boxes))\n",
        "  for box in result.boxes:              #go through all boxes\n",
        "    classIndex=box.cls                  #get class index\n",
        "    confidence=box.conf                 #get confidence\n",
        "    xywh=box.xywh                       #get bounding box\n",
        "    print('    class index: ',classIndex)\n",
        "    print('    confidence: ',confidence)\n",
        "    print('    xywh: ',xywh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ0uvfPHH91Y"
      },
      "source": [
        "##Processing the Result\n",
        "\n",
        "The results are \"tensors\".\n",
        "The following code extract the values and store them into 2 lists: 1 for the images, and 1 for the boxes detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ODZubcQdmd"
      },
      "outputs": [],
      "source": [
        "#minimum confidence required\n",
        "confidenceThreshold=0.5\n",
        "\n",
        "#lists to collect results\n",
        "originalImages=[]\n",
        "detectionResults=[]\n",
        "\n",
        "for result in results:\n",
        "  img=result.orig_img       #get original image\n",
        "  originalImages.append(img)  #append image to list\n",
        "  boxResults=[]\n",
        "  for box in result.boxes:  #go through all boxes\n",
        "    #\n",
        "    #compute values from tensors\n",
        "    #\n",
        "    [classIndex]=box.cls.cpu().numpy()    #get class index of box\n",
        "    classIndex=int(classIndex)            #convert to int\n",
        "    label=classNames[classIndex]          #get class name as label\n",
        "    [confidence]=box.conf.cpu().numpy()   #get box confidence\n",
        "    label=label+' '+str(confidence)       #append confidence to label\n",
        "    [[x,y,w,h]]=box.xywh.cpu().numpy()    #get box centre, width and height\n",
        "    topx=int(x-w/2)       #compute box top left corner\n",
        "    topy=int(y-h/2)\n",
        "    bottomx=int(topx+w)   #compute box bottom right corner\n",
        "    bottomy=int(topy+h)\n",
        "    #\n",
        "    #append box result to list\n",
        "    #\n",
        "    if confidence>=confidenceThreshold:\n",
        "      boxResults.append((classIndex,confidence,label,(topx,topy,bottomx,bottomy)))\n",
        "  detectionResults.append(boxResults)\n",
        "\n",
        "#\n",
        "#print out computed results\n",
        "#\n",
        "for boxResults in detectionResults:\n",
        "  print('Image detection result:')\n",
        "  boxIndex=0\n",
        "  for boxResult in boxResults:\n",
        "    (classIndex,confidence,label,(topx,topy,bottomx,bottomy))=boxResult\n",
        "    print('  box no.: ',boxIndex)\n",
        "    print('    class index: ',classIndex)\n",
        "    print('    confidence: ',confidence)\n",
        "    print('    label: ',label)\n",
        "    print('    box: ',(topx,topy,bottomx,bottomy))\n",
        "    boxIndex=boxIndex+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpUSLjpgKATu"
      },
      "source": [
        "##Visualising the Results\n",
        "\n",
        "We can now visualise the bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y73sN2FlXlhT"
      },
      "outputs": [],
      "source": [
        "#font to be used in label\n",
        "font = cv.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#generate random colours for the classes\n",
        "colours = np.random.uniform(0, 255, size=(len(classNames), 3))\n",
        "\n",
        "#\n",
        "#go through all images\n",
        "#\n",
        "for index in range(0,len(originalImages)):\n",
        "  image=originalImages[index]         #take out the image\n",
        "  boxResults=detectionResults[index]  #get all box results of this image\n",
        "  for boxResult in boxResults:        #go through all boxes in this result\n",
        "    (classIndex,confidence,label,(topx,topy,bottomx,bottomy))=boxResult\n",
        "    colour = colours[classIndex]                 #get colour\n",
        "    cv.rectangle(image, (topx,topy), (bottomx,bottomy), colour, 2)  #draw bounding box\n",
        "    cv.putText(image, label, (topx,topy -10), font, 0.5, colour)    #draw class label\n",
        "  #\n",
        "  #show image\n",
        "  #\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Non-max Suppression\n",
        "\n",
        "In the bird photo, you may notice that the same bird is detected twice with overlapping boxes.\n",
        "We can do non-max suppression to post-process the detection/prediction result to remove duplicate detections. While the Ultralytics library has a [`non_max_suppression(...)` function](https://docs.ultralytics.com/reference/utils/ops/#ultralytics.utils.ops.non_max_suppression), I cannot figure out how to use it.\n",
        "Instead, I will [turn on non-max suppression](https://docs.ultralytics.com/modes/predict/#inference-arguments) during the detection/prediction process.\n",
        "\n",
        "Notice the number of detected objects/boxes in each photo after non-max suppression is turned on."
      ],
      "metadata": {
        "id": "eeskrLREawCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#predict with non-max suppression turned on\n",
        "#confidence threshold=0.5\n",
        "#iou threshold=0.6\n",
        "#\n",
        "results=model.predict(images,agnostic_nms=True,conf=0.5,iou=0.6)"
      ],
      "metadata": {
        "id": "Yo1qQJYbauBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualising Results with Non-max Suppression\n",
        "\n",
        "To show our \"non-max suppressed\" results, we use code similar to the above.\n",
        "\n",
        "Notice that I have put in the minimum confidence value in the prediction/detection code. So I can skip the test in processing the result."
      ],
      "metadata": {
        "id": "RvtEM4Zmr_Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#same code as above to process results\n",
        "#\n",
        "\n",
        "#lists to collect results\n",
        "originalImages=[]\n",
        "detectionResults=[]\n",
        "\n",
        "for result in results:\n",
        "  img=result.orig_img       #get original image\n",
        "  originalImages.append(img)  #append image to list\n",
        "  boxResults=[]\n",
        "  for box in result.boxes:  #go through all boxes\n",
        "    #\n",
        "    #compute values from tensors\n",
        "    #\n",
        "    [classIndex]=box.cls.cpu().numpy()    #get class index of box\n",
        "    classIndex=int(classIndex)            #convert to int\n",
        "    label=classNames[classIndex]          #get class name as label\n",
        "    [confidence]=box.conf.cpu().numpy()   #get box confidence\n",
        "    label=label+' '+str(confidence)       #append confidence to label\n",
        "    [[x,y,w,h]]=box.xywh.cpu().numpy()    #get box centre, width and height\n",
        "    topx=int(x-w/2)       #compute box top left corner\n",
        "    topy=int(y-h/2)\n",
        "    bottomx=int(topx+w)   #compute box bottom right corner\n",
        "    bottomy=int(topy+h)\n",
        "    #\n",
        "    #append box result to list\n",
        "    #\n",
        "    boxResults.append((classIndex,confidence,label,(topx,topy,bottomx,bottomy)))\n",
        "  detectionResults.append(boxResults)\n",
        "\n",
        "#\n",
        "#same code as above to show images and boxes\n",
        "#\n",
        "\n",
        "#font to be used in label\n",
        "font = cv.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#generate random colours for the classes\n",
        "colours = np.random.uniform(0, 255, size=(len(classNames), 3))\n",
        "\n",
        "#\n",
        "#go through all images\n",
        "#\n",
        "for index in range(0,len(originalImages)):\n",
        "  image=originalImages[index]         #take out the image\n",
        "  boxResults=detectionResults[index]  #get all box results of this image\n",
        "  for boxResult in boxResults:        #go through all boxes in this result\n",
        "    (classIndex,confidence,label,(topx,topy,bottomx,bottomy))=boxResult\n",
        "    colour = colours[classIndex]                 #get colour\n",
        "    cv.rectangle(image, (topx,topy), (bottomx,bottomy), colour, 2)  #draw bounding box\n",
        "    cv.putText(image, label, (topx,topy -10), font, 0.5, colour)    #draw class label\n",
        "  #\n",
        "  #show image\n",
        "  #\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n"
      ],
      "metadata": {
        "id": "Thjt7Zi3rchW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBY3fFiNeh5KdIRf68oXy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}