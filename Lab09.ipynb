{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEPZigjTgDBARm32h7hAbS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CM4709 Computer Vision\n",
        "#Lab 09 Time Series Prediction Using LSTM\n",
        "\n",
        "Note: This lab is based on the [Youtube video here](https://youtu.be/c0k-YLQGKjY).\n",
        "\n",
        "##Aims\n",
        "* Use the Tensorflow LSTM library/model to predict a time series.\n",
        "\n",
        "##Downloading Dataset\n",
        "\n",
        "The [Jena climate dataset](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) is a time series that consists of 14 features like temperature, pressure, humidity, etc.\n",
        "We will use the dataset to predict temperature only.\n",
        "The data set is huge, with data recorded every 10 minutes for 8 years from 2009 to 2016.\n",
        "\n",
        "The following code (adapted from the Youtube video) download the dataset:"
      ],
      "metadata": {
        "id": "WC2uDTBP_tl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Zh97lv-4ch"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "print(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can use the following shell commands to download and unzip the dataset.\n",
        "The path to the CSV file will be different, but it is fine as far as the `csv_path` value is set correctly:"
      ],
      "metadata": {
        "id": "wPvMhzq0T9gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip\n",
        "csv_path='jena_climate_2009_2016.csv'"
      ],
      "metadata": {
        "id": "1WQLggjiTWCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploring the Dataset\n",
        "\n",
        "The following codes read the CSV file into a dataframe and dump out the data:"
      ],
      "metadata": {
        "id": "4tD2Y6LjUMsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_original=pd.read_csv(csv_path)\n",
        "df_original"
      ],
      "metadata": {
        "id": "Yr2c8TdU_FEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is huge, as data were logged with 10 minutes for 8 years.\n",
        "We do not need such a high resolution sampling, and will only take data at every hour.\n",
        "The following code will get data from the original dataframe from row 5, and every 6 rows. This will be the data at every hour:\n"
      ],
      "metadata": {
        "id": "JrvgsaRS08Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_original[5::6]\n",
        "df"
      ],
      "metadata": {
        "id": "BT2bcT2S1kFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now set the dataframe index to be the timestamp, with a different format:"
      ],
      "metadata": {
        "id": "TqPHm66u2Ki7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.index = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "YF4yb5jT_FQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the temperature column and plot it:"
      ],
      "metadata": {
        "id": "dsyp6Olx_zRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = df['T (degC)']\n",
        "temp.plot()"
      ],
      "metadata": {
        "id": "Ak3htb5R_FYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare Input and Label\n",
        "\n",
        "To predict the temperature at time t, we will use the 5 previous temperature data at time t-5, t-4, t-3, t-2 and t-1.\n",
        "We need to reshape our temperature column value into the correct shape."
      ],
      "metadata": {
        "id": "LufvhrGdAGoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# [[[1], [2], [3], [4], [5]]] [6]\n",
        "# [[[2], [3], [4], [5], [6]]] [7]\n",
        "# [[[3], [4], [5], [6], [7]]] [8]\n",
        "\n",
        "def df_to_x_y(df, window_size=5):\n",
        "  df_as_np = df.to_numpy()\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(len(df_as_np)-window_size):\n",
        "    row = [[a] for a in df_as_np[i:i+window_size]]\n",
        "    x.append(row)\n",
        "    label = df_as_np[i+window_size]\n",
        "    y.append(label)\n",
        "  return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "StUIWGNy_5cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 5\n",
        "x1, y1 = df_to_x_y(temp, WINDOW_SIZE)\n",
        "print('x shape: ',x1.shape)\n",
        "print('y shape: ',y1.shape)\n",
        "print('===X===')\n",
        "print(x1[:5])\n",
        "print('===Y===')\n",
        "print(y1[:5])"
      ],
      "metadata": {
        "id": "7GRL0KYv_5gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training, Validation, and Testing Datasets\n",
        "\n",
        "We will split the dataset into training, validation, and testing."
      ],
      "metadata": {
        "id": "1zk6VC5OALeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#take 1st 60000 entries as training data\n",
        "x_train1, y_train1 = x1[:60000], y1[:60000]\n",
        "\n",
        "#next 5000 entries as validation data\n",
        "x_val1, y_val1 = x1[60000:65000], y1[60000:65000]\n",
        "\n",
        "#the rest as testing data\n",
        "x_test1, y_test1 = x1[65000:], y1[65000:]\n",
        "\n",
        "#print shapes of all subsets\n",
        "print('x_train shape: ',x_train1.shape)\n",
        "print('y_train shape: ', y_train1.shape)\n",
        "print('x_val shape: ',x_val1.shape)\n",
        "print('y_val shape: ',y_val1.shape)\n",
        "print('x_test shape:',x_test1.shape)\n",
        "print('y_test shape:', y_test1.shape)"
      ],
      "metadata": {
        "id": "jSImFVYH_5jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the LSTM Model\n",
        "\n",
        "We can now create the LSTM model.\n",
        "The process is similar to how we create other NN.\n",
        "Note the use of the [`LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/) layer.\n",
        "Notice the shape of the input and output layers.\n",
        "\n",
        "Also note that LSTM is keeping memory of the whole sequence. So the output is not only dependent on the input values at an instance, but all previous inputs."
      ],
      "metadata": {
        "id": "Bhd5R_jiAXu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(InputLayer((5, 1)))\n",
        "model1.add(LSTM(64))\n",
        "model1.add(Dense(8, 'relu'))\n",
        "model1.add(Dense(1, 'linear'))\n",
        "\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "PgQ2BQbMAU5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compile & Train Model\n",
        "\n",
        "Training of the model is quite standard.\n",
        "We add a callback to create a checkpoint that saves the best model during validation."
      ],
      "metadata": {
        "id": "_p99qpFyA0rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#callback for saving best model during training\n",
        "cp1 = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "\n",
        "#compile model\n",
        "model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])\n",
        "\n",
        "#train model\n",
        "model1.fit(x_train1, y_train1, validation_data=(x_val1, y_val1), epochs=10, callbacks=[cp1])"
      ],
      "metadata": {
        "id": "BWynC_KzArUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction vs Actual in Training Dataset\n",
        "\n",
        "In most cases we won't do this as the model is trained using the training data. It should perform well on this."
      ],
      "metadata": {
        "id": "7gMkw8jQDarT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#load the best model\n",
        "model1 = load_model('best_model.keras')"
      ],
      "metadata": {
        "id": "bIQeKLr_A9tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do predict on training data\n",
        "train_predictions = model1.predict(x_train1).flatten()\n",
        "\n",
        "#print prediction vs actual\n",
        "train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train1})\n",
        "train_results"
      ],
      "metadata": {
        "id": "CuxR-3zVBFJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot Prediction vs Actual in Training Dataset"
      ],
      "metadata": {
        "id": "T6i01BIsBIuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_results['Train Predictions'][:100])\n",
        "plt.plot(train_results['Actuals'][:100])\n"
      ],
      "metadata": {
        "id": "eII98Vn-BH1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction vs Actual in Validation Dataset"
      ],
      "metadata": {
        "id": "-ZPjXIj1CtMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = model1.predict(x_val1).flatten()\n",
        "val_results = pd.DataFrame(data={'Val Predictions':val_predictions, 'Actuals':y_val1})\n",
        "val_results"
      ],
      "metadata": {
        "id": "AAa20Oo9BPf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val_results['Val Predictions'][:100])\n",
        "plt.plot(val_results['Actuals'][:100])"
      ],
      "metadata": {
        "id": "T22bVzK3BTJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction vs Actual in Testing Dataset"
      ],
      "metadata": {
        "id": "vm2HPRh-DIqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model1.predict(x_test1).flatten()\n",
        "test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':y_test1})\n",
        "test_results"
      ],
      "metadata": {
        "id": "5yGZ_pY2BW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_results['Test Predictions'][:100])\n",
        "plt.plot(test_results['Actuals'][:100])"
      ],
      "metadata": {
        "id": "ipCoSyfMBaDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Other Prediction Techniques\n",
        "\n",
        "If you are interested to go further, there is a [Part 2 of the Youtube video](https://youtu.be/kGdbPnMCdOg) which uses other techniques like GRU and 1d CNN."
      ],
      "metadata": {
        "id": "_roQq6HbDTnh"
      }
    }
  ]
}