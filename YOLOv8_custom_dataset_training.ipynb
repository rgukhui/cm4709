{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgukhui/cm4709/blob/main/YOLOv8_custom_dataset_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training YOLOv8 on a Custom Dataset\n",
        "\n",
        "By default, you can find weights of YOLOv8 trained on the COCO dataset which includes 80 classes of common objects in daily life.\n",
        "If you have a specialised domain, you may want to train YOLO on a custom dataset.\n",
        "\n",
        "##Aims\n",
        "1. To create a custom dataset from videos using Roboflow.\n",
        "1. To annotate a custom dataset using Roboflow.\n",
        "1. To train YOLOv8 with a custom dataset.\n",
        "1. Using a trained YOLOv8 network for object detection."
      ],
      "metadata": {
        "id": "iyb_PHpRdtSB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp0NU4lUCX46"
      },
      "source": [
        "#Connect to Google Drive\n",
        "\n",
        "As usual, we don't want to save files in the Colab runtime as they will get deleted when the runtime is disconnected. e.g. weights of the neural network after training.\n",
        "It is recommended to save files our GoogleDrive space and mount it to your runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJulYYDvZoE",
        "outputId": "cdf0a465-b6e4-46a0-f34c-da0e1ce148b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount Google drive to /content/drive\n",
        "#\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCRHyABvVGa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Roboflow for Data Annotation\n",
        "\n",
        "Roboflow is a free image annotation tool. Before using it, please be aware of the followings:\n",
        "1. By default, datasets uploaded to Roboflow will be made public. **If your data are confidential, do not use Roboflow**.\n",
        "1. With a free account, there is a limit on the number of images in the dataset.\n",
        "\n",
        "##Creating a Roboflow Project\n",
        "1. Log into your Google account as usual.\n",
        "1. Go to [www.roboflow.com](http://www.roboflow.com). If it is the first time you use Roboflow, you can link it to your Google Account. Alternatively you can create a separate Roboflow account.\n",
        "1. After signing into Roboflow, \"Create a New Project\".\n",
        "1. Give the project a name. For this demo, we will create an \"Object Detection\" dataset.\n",
        "\n",
        "##Creating Images from Video\n",
        "While you can upload individual images to Roboflow, an easier way is to upload a video and ask Roboflow to sample frames from the video.\n",
        "\n",
        "1. Drag-and-drop a video into your Roboflow project.\n",
        "1. Choose the sample/frame-rate. Roboflow will tell you how many images will be sampled from the video. Usually you choose the frame rate depending on the nature of the video. e.g. If it contains fast-moving objects, you may want to use a higher frame rate.\n",
        "1. Click the \"Choose Frame Rate\" button for Roboflow to complete the sampling and upload all images to your project.\n",
        "\n",
        "##Assigning Annotation Task to People\n",
        "Image annotation is usually the most time-consuming task as it require manual labelling (and also bounding box drawing in Object Detection). Roboflow allows you to assign annotation of an image subset to different people.\n",
        "\n",
        "Roboflow also has an \"Auto labelling\" function, but for this simple demo we will do \"Manual Labelling\".\n",
        "\n",
        "1. Click \"Manual Labelling\".\n",
        "1. Assign all images to yourself for annotation.\n",
        "1. Click \"Assign Images\" to complete the assignment.\n",
        "\n",
        "##Annotation of Images\n",
        "This is the most time-consuming task: For every image, you need to draw the bounding box of all \"objects of interest\" and label them with classes.\n",
        "\n",
        "1. Click the \"Start Annotating\" button to start.\n",
        "1. For each image: If there is no object, mark it as \"Null\". If there are objects, draw the bounding box and give the object a class label.\n",
        "\n",
        "##Generate a Dataset\n",
        "You can create different versions of datasets from the same set of images. e.g. Image sizes can be different, and you can apply pro-processing and augmentation.\n",
        "\n",
        "1. Click \"Generate\" on the sidebar.\n",
        "1. Add any pre-processing you want. e.g. Resize the image to 416x416 for YOLOv8, or convert images to greyscale if colour information is not important.\n",
        "1. Optionally, add \"Augmentation\" to the images to cover more situations. e.g. flipping images horizontal. You may not want to flip vertically as in some domains, objects may never appear up-side-down. (Note that augmentation creates more images, which may exceed the image count in a free project.)\n",
        "\n",
        "##Exporting a Dataset\n",
        "Now your dataset is ready, you can export it to be used in Colab. We will come back to this later in Colab.\n"
      ],
      "metadata": {
        "id": "SF0jDYqZfIqH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8_-fU3UCcQz"
      },
      "source": [
        "#Installing the YOLOv8 Library\n",
        "\n",
        "To use YOLOv8 in Colab, we need to install the libraries from Ultralytics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDdVjq_RR9AQ"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Dataset from Roboflow\n",
        "\n",
        "To download/import a dataset from Roboflow:\n",
        "1. Go to Roboflow, your project, dataset, and click \"Versions\" on the side-bar.\n",
        "1. Click \"Export Dataset\" button.\n",
        "1. Select \"YOLO V8\" format, \"show download code\", and \"Continue\".\n",
        "1. Copy the code and paste it into the code cell below.\n",
        "\n",
        "Note: The generate code contains your Roboflow API key. **DO NOT disclose your API key to others.**\n",
        "\n",
        "If you only want to try training YOLO but do not bother spending the time to annotate a dataset, you can use my [bird camera dataset](https://universe.roboflow.com/bird-hiao0/bird-camera)."
      ],
      "metadata": {
        "id": "XcrNd3M_S75P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#make folder to store dataset\n",
        "#and change directory to there before downloading\n",
        "#\n",
        "%mkdir /content/datasets\n",
        "%cd /content/datasets\n",
        "\n",
        "#\n",
        "#the follow code is copied from Roboflow datset export\n",
        "#Note: It contains an API key which you MUST NOT show to others.\n",
        "#\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"***Roboflow API key here***\")\n",
        "project = rf.workspace(\"bird-hiao0\").project(\"bird-camera\")\n",
        "version = project.version(8)\n",
        "dataset = version.download(\"yolov8\")\n"
      ],
      "metadata": {
        "id": "7fGjaGU_TAUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fixing the `data.yaml` File\n",
        "\n",
        "The downloaded dataset is in the `/content/datasets` folder of your Colab runtime.\n",
        "\n",
        "The `data.yaml` file specifies parameters of the dataset. Unfortunately, the generated `data.yaml` is not quite correct. In particular, the paths to the training and validation subsets are wrong. We need to fix 2 errors before we can use the dataset:\n",
        "\n",
        "1. In Colab, look for the `data.yaml` file under the `datasets/xxxx-nn` folder, where `xxxx` is your Roboflow project name, and `nn` is the dataset version number.\n",
        "1. Look for 2 lines that look similar to this:\n",
        "\n",
        "```\n",
        "train: xxxx-nn/train/images\n",
        "val: xxxx-nn/valid/images\n",
        "```\n",
        "\n",
        "Change them to the followings:\n",
        "\n",
        "```\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "```\n"
      ],
      "metadata": {
        "id": "sFKw2BdEVhUz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D5YVqwJRGUd"
      },
      "source": [
        "#Training YOLO V8\n",
        "\n",
        "With the YOLO V8 library and the dataset ready, we can now train our model. You may want to modify the following parameters:\n",
        "* `model`: This is the YOLO model to use. This defines the size and scale of the network.\n",
        "* `epochs`: The number of epochs to train.\n",
        "* `imgsz`: Input image size to YOLO. V8 default size is 640.\n",
        "\n",
        "See the Ultralytics documentation [here](https://docs.ultralytics.com/modes/train/#key-features-of-train-mode) on how to train and the meaning of these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fc4XlmKT2dR"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#train YOLO using yolov8n model, 50 epoch, image size 416\n",
        "#\n",
        "# Note: Modify \"data\" value to point to \"data.yaml\" in dataset.\n",
        "#\n",
        "!yolo task=detect mode=train model=yolov8n.pt data=/content/datasets/bird-camera-8/data.yaml epochs=50 imgsz=416 plots=True\n",
        "\n",
        "#\n",
        "#Optionally, copy best weights into Google Drive space after training\n",
        "#\n",
        "#Note: Weight files are put into subfolder \"train\", \"train2\", etc. in subsequent training runs.\n",
        "#\n",
        "!mkdir /content/drive/MyDrive/YOLO/v8/weights\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/drive/MyDrive/YOLO/v8/weights/yolov8n.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Object Detection from Command Line\n",
        "\n",
        "To do object detection in Command Line, we only need the paths to the weights file and the image.\n"
      ],
      "metadata": {
        "id": "mwySVXzupbUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect predict model=/content/runs/detect/train/weights/best.pt source='/content/drive/MyDrive/cm4709/YOLOv8 training demo/images/robin1.png'"
      ],
      "metadata": {
        "id": "Dc4j6NSUp18x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Object Detection Using PyTorch\n",
        "\n",
        "The following codes ar mostly from Lab06 in using YOLO v8 in PyTorch:"
      ],
      "metadata": {
        "id": "u-WpEEHermZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#our testing images\n",
        "\n",
        "image1='/content/drive/MyDrive/cm4709/YOLOv8 training demo/images/bluetit1.png'\n",
        "image2='/content/drive/MyDrive/cm4709/YOLOv8 training demo/images/bluetit2.png'\n",
        "image3='/content/drive/MyDrive/cm4709/YOLOv8 training demo/images/robin1.png'\n",
        "image4='/content/drive/MyDrive/cm4709/YOLOv8 training demo/images/robin2.png'\n",
        "images=[image1,image2,image3,image4]\n",
        "\n",
        "#weights file path\n",
        "weightsFile='/content/runs/detect/train/weights/best.pt'\n",
        "\n",
        "model = YOLO(weightsFile)  # load a pretrained YOLOv8s detection model\n",
        "results=model.predict(images,agnostic_nms=True,iou=0.6)  # predict on images\n",
        "result=results[0]     #get 1st result\n",
        "classNames=result.names #get class names\n",
        "\n",
        "print('No. of classes: ',len(classNames))\n",
        "print(classNames)"
      ],
      "metadata": {
        "id": "Vn1Wikt-rqEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#same code as above to process results\n",
        "#\n",
        "\n",
        "#lists to collect results\n",
        "originalImages=[]\n",
        "detectionResults=[]\n",
        "\n",
        "for result in results:\n",
        "  img=result.orig_img       #get original image\n",
        "  originalImages.append(img)  #append image to list\n",
        "  boxResults=[]\n",
        "  for box in result.boxes:  #go through all boxes\n",
        "    #\n",
        "    #compute values from tensors\n",
        "    #\n",
        "    [classIndex]=box.cls.cpu().numpy()    #get class index of box\n",
        "    classIndex=int(classIndex)            #convert to int\n",
        "    label=classNames[classIndex]          #get class name as label\n",
        "    [confidence]=box.conf.cpu().numpy()   #get box confidence\n",
        "    label=label+' '+str(confidence)       #append confidence to label\n",
        "    [[x,y,w,h]]=box.xywh.cpu().numpy()    #get box centre, width and height\n",
        "    topx=int(x-w/2)       #compute box top left corner\n",
        "    topy=int(y-h/2)\n",
        "    bottomx=int(topx+w)   #compute box bottom right corner\n",
        "    bottomy=int(topy+h)\n",
        "    #\n",
        "    #append box result to list\n",
        "    #\n",
        "    boxResults.append((classIndex,confidence,label,(topx,topy,bottomx,bottomy)))\n",
        "  detectionResults.append(boxResults)\n",
        "\n",
        "#\n",
        "#same code as above to show images and boxes\n",
        "#\n",
        "\n",
        "#font to be used in label\n",
        "font = cv.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#generate random colours for the classes\n",
        "colours = np.random.uniform(0, 255, size=(len(classNames), 3))\n",
        "\n",
        "#\n",
        "#go through all images\n",
        "#\n",
        "for index in range(0,len(originalImages)):\n",
        "  image=originalImages[index]         #take out the image\n",
        "  boxResults=detectionResults[index]  #get all box results of this image\n",
        "  for boxResult in boxResults:        #go through all boxes in this result\n",
        "    (classIndex,confidence,label,(topx,topy,bottomx,bottomy))=boxResult\n",
        "    colour = colours[classIndex]                 #get colour\n",
        "    cv.rectangle(image, (topx,topy), (bottomx,bottomy), colour, 2)  #draw bounding box\n",
        "    cv.putText(image, label, (topx,topy -10), font, 0.5, colour)    #draw class label\n",
        "  #\n",
        "  #show image\n",
        "  #\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "IBM7cnApsWby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMr6W4PQ-Vq"
      },
      "source": [
        "# Export Weights to ONXX\n",
        "\n",
        "After training, the \"weights\" file is in PyTorch format. To use it in other platforms (e.g. OpenCV), you can export the `.pt` weight file into `.onnx` ([Open Neural Network Exchange](https://onnx.ai/)) format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8It43QuWYxFa"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Change the weight file path accordingly.\n",
        "#\n",
        "#Note: OpenCV requires the use of \"opset=12\" and \"simplify\" for compatibility.\n",
        "#\n",
        "!yolo task=detect mode=export model=/content/runs/detect/train/weights/best.pt format=onnx opset=12 simplify imgsz=416"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMnuQClrts1blQcq5f/djwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}