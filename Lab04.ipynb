{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc6KaLQpAZDucj7ChIrHX6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CM4709 Computer Vision\n",
        "#Lab 04 Convolutional Neural Network\n",
        "\n",
        "##Aims\n",
        "1. Use the Keras library to build a Convolutional Neural Network (CNN).\n",
        "1. Use a CNN to classify images in the Fashion MNIST dataset.\n",
        "\n",
        "\n",
        "##The Fashion MNIST Dataset\n",
        "\n",
        "In Lab 03, we used a Neural Network to classify images in the [fashion MNIST dataset](https://www.kaggle.com/datasets/zalando-research/fashionmnist).\n",
        "This week, we will use a Convolutional Neural Network (CNN) to do the same job.\n",
        "We will compare the performance of these 2 approaches before any tuning.\n",
        "\n",
        "The following code is from Lab 03.\n",
        "It simply loads the Fashion MNIST data and prepare the training, validation, and testing datasets.\n",
        "\n",
        "There is no change from last week:\n"
      ],
      "metadata": {
        "id": "1dKSC_pRqAme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#load dataset from Keras\n",
        "(x_train_full,y_train_full),(x_test,y_test)=keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#split full training dataset into validation and training subsets\n",
        "x_valid=x_train_full[:5000]/255.0\n",
        "x_train=x_train_full[5000:]/255.0\n",
        "y_valid=y_train_full[:5000]\n",
        "y_train=y_train_full[5000:]\n",
        "\n",
        "print('Full training dataset (images): ',x_train_full.shape)\n",
        "print('Full training dataset (labels): ',y_train_full.shape)\n",
        "print('Training dataset (images): ',x_train.shape)\n",
        "print('Training dataset (labels): ',y_train.shape)\n",
        "print('Validation dataset (images): ',x_valid.shape)\n",
        "print('Validation dataset (labels): ',y_valid.shape)\n",
        "print('Testing dataset (images): ',x_test.shape)\n",
        "print('Testing dataset (labels): ',y_test.shape)\n"
      ],
      "metadata": {
        "id": "MGbyIS0Y2FXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the Neural Network\n",
        "\n",
        "The following code builds a neural network of 4 layers.\n",
        "1. We use a [Sequential model](https://keras.io/guides/sequential_model/).\n",
        "1. The input will feed into a [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layer.\n",
        "1. This is followed by a [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) layer.\n",
        "1. Then another [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layer.\n",
        "1. And another [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) layer.\n",
        "1. Then we flatten the layer.\n",
        "1. Fit it into a [Dense](https://keras.io/api/layers/core_layers/dense/) layer.\n",
        "1. Finally the output layer has 1 neurons, corresponding to the 10 classes in the dataset. Again, we use the [Softmax activation function](https://keras.io/api/layers/activations/#softmax-function) in the output layer."
      ],
      "metadata": {
        "id": "rmO3L5yXqsNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#create a neural network using Keras\n",
        "model=keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3),activation='relu', padding='same',input_shape=(28,28,1)))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3),activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Densely connected layers\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "o_S58ieqrCge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions\n",
        "Look at different layers in the CNN:\n",
        "1. What is the size of the kernel in the 1st convolutional layer?\n",
        "1. How many kernels do we have in the 1st conv layer?\n",
        "1. How many parameters are in the 1st conv layer? What do they correspond to?\n",
        "1. What is the size of the 1st pooling layer?\n",
        "1. Compare the number of parameters in a NN. What do you notice?\n"
      ],
      "metadata": {
        "id": "dumXoUT0q0SI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the Labels\n",
        "\n",
        "There are 10 classes in the dataset.\n",
        "The output layer, with 10 neurons, expect a hot-one encoding.\n",
        "For example, an output label of `4` is expected in the form of a vector/array: `[0,0,0,1,0,0,0,0,0,0]`.\n",
        "\n",
        "The following code show you the shape of the training dataset labels, and the first element value:"
      ],
      "metadata": {
        "id": "p6Z_PJ2Izv23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('y_train shape: ',y_train.shape)\n",
        "print('y_train[0]: ',y_train[0])"
      ],
      "metadata": {
        "id": "a9vwYSo90XdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code converts the output datasets into the expected format/encoding. We use a utility function [`keras.utils.categorical(...)`](https://keras.io/api/utils/python_utils/#tocategorical-function):"
      ],
      "metadata": {
        "id": "9qNMYySh0yFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_cat=keras.utils.to_categorical(y_train)\n",
        "y_test_cat=keras.utils.to_categorical(y_test)\n",
        "y_valid_cat=keras.utils.to_categorical(y_valid)\n",
        "\n",
        "print('y_train_cat.shape: ',y_train_cat.shape)\n",
        "print('y_train_cat[0]: ',y_train_cat[0])"
      ],
      "metadata": {
        "id": "OWJ1vmYt0-xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compiling and Training the Model\n",
        "\n",
        "Now we can compile and train the model.\n",
        "This time we are using the [Adam optimizer](https://keras.io/api/optimizers/adam/).\n",
        "Loss function is [categorical_cross_entropy[(https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class).\n",
        "\n",
        "##Questions:\n",
        "1. Look at the output dataset (i.e. `y_train_cat`). Why do you think we use `categorical_crossentropy` as the loss function?"
      ],
      "metadata": {
        "id": "Gvm18-7hq80F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile with adam optimizer & categorical_crossentropy loss function\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics='accuracy')\n",
        "\n",
        "#train model\n",
        "history=model.fit(x_train,y_train_cat,epochs=30,\n",
        "                  validation_data=(x_valid,y_valid_cat))\n"
      ],
      "metadata": {
        "id": "TK7qov0vr5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualising the Training\n",
        "\n",
        "The following code (from Lab 03) shows the graphs of loss, accuracy, validation-loss, and validation-accuracy as the training progressed:"
      ],
      "metadata": {
        "id": "jBihw7WIrIgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "#show keys in the history\n",
        "print(history.history.keys())\n",
        "\n",
        "#plot graphs\n",
        "pd.DataFrame(history.history).plot(figsize=(15,8))\n",
        "pyplot.grid(True)\n",
        "pyplot.gca().set_ylim(0,1)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "NJZq15EFxS25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question\n",
        "1. What do you observe in the graphs?\n",
        "1. Try to explain the observation."
      ],
      "metadata": {
        "id": "NYvILnHoxvg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Model\n",
        "\n",
        "Finally, we use the testing dataset to evaluate the CNN's performance on unseen data:"
      ],
      "metadata": {
        "id": "hINWcFxqY9qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model using the testing dataset\n",
        "(loss,accuracy)=model.evaluate(x_test,y_test_cat)\n",
        "\n",
        "print('Loss: ',loss)\n",
        "print('Accuracy: ',round(accuracy,2)*100,'%')"
      ],
      "metadata": {
        "id": "o1UwUuYmyyjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions\n",
        "\n",
        "1. How does the performance of the CNN compare with the NN in Lab 03?\n",
        "1. Suggest ways to improve the model's performance."
      ],
      "metadata": {
        "id": "LFmZ1ALBrOA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Doing Prediction\n",
        "\n",
        "The following code below from Lab 03 takes a random image from the testing dataset and does a prediction of its class:"
      ],
      "metadata": {
        "id": "dUN3QaDB219X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#class names in an array\n",
        "class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "#get a random image from the testing dataset\n",
        "(instance_count,width,height)=x_test.shape    #get dataset size\n",
        "index=random.randint(0,instance_count-1)      #get an index witin dataset\n",
        "image=x_test[index]                           #get image\n",
        "pyplot.imshow(image,cmap='gray')              #show image\n",
        "\n",
        "#need to add 1 dimension to array to fit input shape\n",
        "oneImage=np.expand_dims(image,axis=0)\n",
        "print('Image shape: ',oneImage.shape)\n",
        "\n",
        "#predict image class\n",
        "[prediction]=model.predict(oneImage)\n",
        "print('Prediction output: ',prediction)\n",
        "\n",
        "classIndex=np.argmax(prediction,axis=-1)\n",
        "\n",
        "predictedClass=class_names[classIndex]\n",
        "print('Predicted class: ',predictedClass)\n",
        "print('Known class: ',class_names[y_test[index]])"
      ],
      "metadata": {
        "id": "r-oowJ0HygXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}